{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Machine\n",
    "\n",
    "This lab is a tutorial on belief propagation, one of the many approaches for solving graphical models. The actual goal is to build a model for identifying faults with coffee machines. You're given a graphical model of how a coffee machine works, that connects observations to possible failure modes. Belief propagation is then used to calculate the probability of each failure (marginal probability) given the available evidence (observations). This is broken up as:\n",
    "1. Given the graphical model for the state of a coffee machine learn the distributions from data. This is basic (Bayesian) statistics.\n",
    "2. Implement belief propagation, so you can evaluate the probability of each failure given the available evidence. This is broken down into a number of steps:\n",
    "    1. Conversion from _Bayes net_ to _factor_graph_, as BP is simplest to implement using the later. A factor graph is described in the slides, but is a bipartite graph of factors and random variables.\n",
    "    2. Belief propagation works by passing messages along the edges of a factor graph. Each message depends on other messages having already been sent, so the second step is to work out a valid order to send messages in.\n",
    "    3. Now you need to pass the messages, by implementing two functions: One to pass a message from a RV (random variable) to a factors, and another to pass a message from a factor to a RV.\n",
    "    4. Finally, the above two steps let you pass all of the messages and extract the final belief. There is an extra complication to handle observed random variables. This has been coded for you.\n",
    "3. Identify the most probable failure for a set of broken coffee machines. No marks here, just lets you try the algorithm out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee Machine Data\n",
    "\n",
    "Alongside this notebook you can also download a zip file that contains the data as a `csv` file. Code provided below will load the data directly from the zip file, so you don't have to unzip it.\n",
    "\n",
    "Each row of the file contains a fully observed coffee machine, with the state of every random variable. The random variables are all binary, with `False` represented by `0` and `True` represented by `1`. The variables are:\n",
    "\n",
    "Failures _(you're trying to detect these)_:\n",
    "* 0. `he` - No electricity\n",
    "* 1. `fp` - Fried power supply unit\n",
    "* 2. `fc` - Fried circuit board  \n",
    "* 3. `wr` - Water reservoir empty\n",
    "* 4. `gs` - Group head gasket forms seal  \n",
    "* 5. `dp` - Dead pump  \n",
    "* 6. `fh` - Fried heating element  \n",
    "\n",
    "\n",
    "Mechanism _(these are unobservable)_:\n",
    "* 7. `pw` - Power supply unit works  \n",
    "* 8. `cb` - Circuit board works  \n",
    "* 9. `gw` - Get water out of group head   \n",
    "\n",
    "Diagnostic _(these are the tests a mechanic can run - observable)_:\n",
    "* 10. `ls` - Room lights switch on\n",
    "* 11. `vp` - A voltage is measured across power supply unit\n",
    "* 12. `lo` - Power light switches on\n",
    "* 13. `wv` - Water visible in reservoir\n",
    "* 14. `hp` - Can hear pump\n",
    "* 15. `me` - Makes espresso\n",
    "* 16. `ta` - Makes a hot, tasty espresso\n",
    "\n",
    "Above the number is the column number of the provided data (`dm`, an abbreviation of _data matrix_) and the two letter code is a suggested variable name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee Machine Model\n",
    "\n",
    "For if you are unfamiliar with an espresso coffee machine here is a brief description of how one works (you can ignore this):\n",
    "> The user puts ground coffee into a portafilter (round container with a handle and two spouts at the bottom), tamps it (compacts the coffee down), and clamps the portafilter into the group head at the front of the machine. A gasket (rubber ring) forms a seal between the portafilter and group head. A button is pressed. Water is drawn from a reservoir by a pump into a boiler. In the boiler a heating element raises the waters temperature, before the pump pushes it through the group head and into the portafilter at high pressure. The water is forced through the coffee grinds and makes a tasty espresso.\n",
    "\n",
    "The graphical model (Bayes net) showing how the variables are related is also provided, as `coffee machine.pdf`; here it is given as conditional probabilities:\n",
    "\n",
    "Failures:\n",
    " * `P_he:` $P(\\texttt{no electricity})$\n",
    " * `P_fp:` $P(\\texttt{fried psu})$\n",
    " * `P_fc:` $P(\\texttt{fried circuit board})$\n",
    " * `P_wr:` $P(\\texttt{water reservoir empty})$\n",
    " * `P_gs:` $P(\\texttt{group head gasket seal broken})$\n",
    " * `P_dp:` $P(\\texttt{dead pump})$\n",
    " * `P_fh:` $P(\\texttt{fried heating element})$\n",
    "\n",
    "Mechanism:\n",
    " * `P_pw_he_fp:` $P(\\texttt{psu works}\\enspace|\\enspace\\texttt{no electricity},\\enspace\\texttt{fried psu})$\n",
    " * `P_cb_pw_fc:` $P(\\texttt{circuit board works}\\enspace|\\enspace\\texttt{psu works},\\enspace\\texttt{fried circuit board})$\n",
    " * `P_gw_cb_wr_dp:` $P(\\texttt{get water}\\enspace|\\enspace\\texttt{circuit board works},\\enspace\\texttt{water reservoir empty},\\enspace\\texttt{dead pump})$\n",
    "\n",
    "Diagnostic:\n",
    " * `P_ls_he:` $P(\\texttt{lights switch on}\\enspace|\\enspace\\texttt{no electricity})$\n",
    " * `P_vp_pw:` $P(\\texttt{voltage across psu}\\enspace|\\enspace\\texttt{psu works})$\n",
    " * `P_lo_cb:` $P(\\texttt{power light on}\\enspace|\\enspace\\texttt{circuit board works})$\n",
    " * `P_wv_wr:` $P(\\texttt{water visible}\\enspace|\\enspace\\texttt{water reservoir empty})$\n",
    " * `P_hp_dp:` $P(\\texttt{can hear pump}\\enspace|\\enspace\\texttt{dead pump})$\n",
    " * `P_me_gw_gs:` $P(\\texttt{makes espresso}\\enspace|\\enspace\\texttt{get water},\\enspace\\texttt{group head gasket seal broken})$\n",
    " * `P_ta_me_fh:` $P(\\texttt{tasty}\\enspace|\\enspace\\texttt{makes espresso},\\enspace\\texttt{fried heating element})$\n",
    "\n",
    "Note that while the model is close to what you may guess the probabilities are not absolute, to account for mistakes and unknown failures. For instance, the mechanic may make a mistake while brewing an espresso and erroneously conclude that the machine is broken when it is in fact awesome. The probabilities associated with each failure are not uniform. The data set is roughly $50:50$ between failed/working machines, which is hardly realistic for a real product, but makes this exercise simpler as it avoids the problem of extremely rare failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Helper used below for some pretty printing, to loop all conditional variable combinations...\n",
    "# (feel free to ignore)\n",
    "def loop_conditionals(count):\n",
    "    if count==0:\n",
    "        yield (slice(None),)\n",
    "    \n",
    "    else:\n",
    "        for head in loop_conditionals(count - 1):\n",
    "            yield head + (0,)\n",
    "            yield head + (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The below loads the data; it also includes some helpful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 262144 exemplars, 17 features\n",
      "     Broken machines  = 122603\n",
      "     Working machines = 139541\n"
     ]
    }
   ],
   "source": [
    "# A mapping from the suggested variable names to column indices in the provided file...\n",
    "nti = dict() # 'name to index'\n",
    "\n",
    "nti['he'] = 0\n",
    "nti['fp'] = 1\n",
    "nti['fc'] = 2\n",
    "nti['wr'] = 3\n",
    "nti['gs'] = 4\n",
    "nti['dp'] = 5\n",
    "nti['fh'] = 6\n",
    "\n",
    "nti['pw'] = 7\n",
    "nti['cb'] = 8\n",
    "nti['gw'] = 9\n",
    "\n",
    "nti['ls'] = 10\n",
    "nti['vp'] = 11\n",
    "nti['lo'] = 12\n",
    "nti['wv'] = 13\n",
    "nti['hp'] = 14\n",
    "nti['me'] = 15\n",
    "nti['ta'] = 16\n",
    "\n",
    "\n",
    "\n",
    "# Opposite to the above - index to name...\n",
    "itn = ['he', 'fp', 'fc', 'wr', 'gs', 'dp', 'fh',\n",
    "       'pw', 'cb', 'gw',\n",
    "       'ls', 'vp', 'lo', 'wv', 'hp', 'me', 'ta'] # 'index to name'\n",
    "\n",
    "\n",
    "\n",
    "# For conveniance this code loads the data from the zip file,\n",
    "# so you don't have to decompress it (takes a few seconds to run)...\n",
    "with zipfile.ZipFile('coffee_machines.zip') as zf:\n",
    "    with zf.open('coffee_machines.csv') as f:\n",
    "        sf = io.TextIOWrapper(f)\n",
    "        reader = csv.reader(sf)\n",
    "        next(reader)\n",
    "        dm = []\n",
    "        for row in reader:\n",
    "            dm.append([int(v) for v in row])\n",
    "        dm = numpy.array(dm, dtype=numpy.int8)\n",
    "\n",
    "\n",
    "\n",
    "# Basic information...\n",
    "print('Data: {} exemplars, {} features'.format(dm.shape[0], dm.shape[1]))\n",
    "print('     Broken machines  =', dm.shape[0] - dm[:,nti['ta']].sum())\n",
    "print('     Working machines =', dm[:,nti['ta']].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Storage\n",
    "\n",
    "The below defines storage to represent the probability distributions that are needed to complete the graphical model. You will be filling them in question 1 (below). A further variable contains a computer readable representation of these variables, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of variables that will ultimately represent conditional probability distributions.\n",
    "# The naming convention is that P_he contains P(he), that P_ta_me_hw contains P(ta|me,hw) etc.\n",
    "# Indexing is always in the same order that the variables are given in the variables name.\n",
    "\n",
    "P_he          = numpy.zeros(2)\n",
    "P_fp          = numpy.zeros(2)\n",
    "P_fc          = numpy.zeros(2)\n",
    "P_wr          = numpy.zeros(2)\n",
    "P_gs          = numpy.zeros(2)\n",
    "P_dp          = numpy.zeros(2)\n",
    "P_fh          = numpy.zeros(2)\n",
    "\n",
    "P_pw_he_fp    = numpy.zeros((2,2,2))\n",
    "P_cb_pw_fc    = numpy.zeros((2,2,2))\n",
    "P_gw_cb_wr_dp = numpy.zeros((2,2,2,2))\n",
    "\n",
    "P_ls_he       = numpy.zeros((2,2))\n",
    "P_vp_pw       = numpy.zeros((2,2))\n",
    "P_lo_cb       = numpy.zeros((2,2))\n",
    "P_wv_wr       = numpy.zeros((2,2))\n",
    "P_hp_dp       = numpy.zeros((2,2))\n",
    "P_me_gw_gs    = numpy.zeros((2,2,2))\n",
    "P_ta_me_fh    = numpy.zeros((2,2,2))\n",
    "\n",
    "\n",
    "\n",
    "# This list describes the above in a computer readable form,\n",
    "# as the tuple (numpy array, human readable name, list of RVs, kind);\n",
    "# the list of RVs is aligned with the dimensions of the numpy array\n",
    "# and kind is 'F' for failure, 'M' for mechanism and 'D' for diagnostic..\n",
    "rvs = [(P_he, 'P(he)', [nti['he']], 'F'),\n",
    "       (P_fp, 'P(fp)', [nti['fp']], 'F'),\n",
    "       (P_fc, 'P(fc)', [nti['fc']], 'F'),\n",
    "       (P_wr, 'P(wr)', [nti['wr']], 'F'),\n",
    "       (P_gs, 'P(gs)', [nti['gs']], 'F'),\n",
    "       (P_dp, 'P(dp)', [nti['dp']], 'F'),\n",
    "       (P_fh, 'P(fh)', [nti['fh']], 'F'),\n",
    "       (P_pw_he_fp, 'P(pw|he,fp)', [nti['pw'], nti['he'], nti['fp']], 'M'),\n",
    "       (P_cb_pw_fc, 'P(cb|pw,fc)', [nti['cb'], nti['pw'], nti['fc']], 'M'),\n",
    "       (P_gw_cb_wr_dp, 'P(gw|cb,wr,dp)', [nti['gw'], nti['cb'], nti['wr'], nti['dp']], 'M'),\n",
    "       (P_ls_he, 'P(ls|he)', [nti['ls'], nti['he']], 'D'),\n",
    "       (P_vp_pw, 'P(vp|pw)', [nti['vp'], nti['pw']], 'D'),\n",
    "       (P_lo_cb, 'P(lo|cb)', [nti['lo'], nti['cb']], 'D'),\n",
    "       (P_wv_wr, 'P(wv|wr)', [nti['wv'], nti['wr']], 'D'),\n",
    "       (P_hp_dp, 'P(hp|dp)', [nti['hp'], nti['dp']], 'D'),\n",
    "       (P_me_gw_gs, 'P(me|gw,gs)', [nti['me'], nti['gw'], nti['gs']], 'D'),\n",
    "       (P_ta_me_fh, 'P(ta|me,fh)', [nti['ta'], nti['me'], nti['fh']], 'D')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Conditional Probability Distributions\n",
    "\n",
    "Above a set of variables representing conditional probability distributions has been defined. They are to represent a Bernoulli trial for each combination of conditional variables, given as $P(\\texttt{False}|...)$ in `rv[0,...]` and $P(\\texttt{True}|...)$ in `rv[1,...]`. Obviously these two values should sum to $1$; giving the probability of both `True` and `False` is redundant, but makes all of the code much cleaner.\n",
    "\n",
    "Your task is to fill in the distributions with a maximum a posteriori probability (MAP) estimate given the data. The prior to be used for all RVs is a Beta distribution,\n",
    "\n",
    "$$P(x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
    "\n",
    "such that $x$ is the probability of getting a `False` from the Bernoulli draw (note that this is backwards from what you might suspect, because it keeps the arrays in the same order). The hyperparameters for the priors are to be set as $\\alpha = \\beta = 1$. The Beta distribution is _conjugate_, that is when you observe a Bernoulli draw and update using Bayes rule the posterior is also a Beta distribution. This makes the procedure particularly simple:\n",
    "1. Initialise every conditional probability with the hyperparameters $\\alpha$ and $\\beta$\n",
    "2. Update them for every coffee machine in the data set\n",
    "3. Extract the maximum likelihood parameters from the posterior $\\alpha$ and $\\beta$ parameters\n",
    "\n",
    "Writing out the relevant parts of the Bayes rule update for observing a RV, $v = 0$ (`False`), you get\n",
    "\n",
    "$$\\operatorname{Beta}(x | \\alpha_1, \\beta_1) \\propto \\operatorname{Bernoulli}(v = 0 | x)\\operatorname{Beta}(x | s\\alpha_0,\\beta_0)$$\n",
    "\n",
    "$$x^{\\alpha_1-1}(1-x)^{\\beta_1-1} \\propto \\left(x^{(1-v)} (1-x)^v\\right) \\left(x^{\\alpha_0-1}(1-x)^{\\beta_0-1}\\right)$$\n",
    "\n",
    "$$x^{\\alpha_1-1}(1-x)^{\\beta_1-1} \\propto x^1 (1-x)^0 x^{\\alpha_0-1}(1-x)^{\\beta_0-1}$$\n",
    "\n",
    "$$x^{\\alpha_1-1}(1-x)^{\\beta_1-1} \\propto x^{\\alpha_0+1-1}(1-x)^{\\beta_0-1}$$\n",
    "\n",
    "$$\\operatorname{Beta}(x | \\alpha_1, \\beta_1) = \\operatorname{Beta}(x | \\alpha_0+1,\\beta_0)$$\n",
    "\n",
    "Subscripts of the hyperparameters are used to indicate how many data points have been seen; `True` works similarly. Put simply, the result is that you count how many instances exist of each combination, and add $1$ for the hyperparameters. The maximum likelihood is then the expected value, which is\n",
    "\n",
    "$$P(v=\\textrm{False}|\\ldots) = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "\n",
    "It's typical to do all of the above within the conditional probability distribution arrays, using them to represent $\\alpha$ and $\\beta$ then treating step 3 as converting from hyperparameters to expected values.\n",
    "\n",
    "Hints:\n",
    "* The use of `0=False` and `1=True` both in the `dm` array and in the conditional probability distributions is very deliberate.\n",
    "* Do not write unique code for each variable - that will be hundreds of lines of code and very tedious/error prone. It's possible to get all of the marks in 7 lines of code, or less, if you're particularly sneaky.\n",
    "* Remember that you can index a `numpy` array with a tuple; for instance using a tuple comprehension such as `tuple(dm[row,c] for c in columns)` you could index an array with the values of the given `columns` indices, as extracted from the current `row` of the data matrix.\n",
    "* The provided `rvs` array exists to support the above two hints.\n",
    " \n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(he) = [0.99047096 0.00952904]\n",
      "\n",
      "P(fp) = [0.80056152 0.19943848]\n",
      "\n",
      "P(fc) = [0.98004547 0.01995453]\n",
      "\n",
      "P(wr) = [0.89956742 0.10043258]\n",
      "\n",
      "P(gs) = [0.95010414 0.04989586]\n",
      "\n",
      "P(dp) = [0.94968071 0.05031929]\n",
      "\n",
      "P(fh) = [0.97033714 0.02966286]\n",
      "\n",
      "P(pw|he,fp):\n",
      "  P(pw|0,0) = [4.81037502e-06 9.99995190e-01]\n",
      "  P(pw|0,1) = [9.99980683e-01 1.93173257e-05]\n",
      "  P(pw|1,0) = [9.99495714e-01 5.04286435e-04]\n",
      "  P(pw|1,1) = [0.9980695 0.0019305]\n",
      "\n",
      "P(cb|pw,fc):\n",
      "  P(cb|0,0) = [9.99981208e-01 1.87916941e-05]\n",
      "  P(cb|0,1) = [9.99048525e-01 9.51474786e-04]\n",
      "  P(cb|1,0) = [4.90910787e-06 9.99995091e-01]\n",
      "  P(cb|1,1) = [9.99760937e-01 2.39062874e-04]\n",
      "\n",
      "P(gw|cb,wr,dp):\n",
      "  P(gw|0,0,0) = [9.99980048e-01 1.99517168e-05]\n",
      "  P(gw|0,0,1) = [9.99611349e-01 3.88651380e-04]\n",
      "  P(gw|0,1,0) = [9.99817718e-01 1.82282173e-04]\n",
      "  P(gw|0,1,1) = [0.99630996 0.00369004]\n",
      "  P(gw|1,0,0) = [5.75251529e-06 9.99994247e-01]\n",
      "  P(gw|1,0,1) = [9.99892404e-01 1.07596299e-04]\n",
      "  P(gw|1,1,0) = [0.90239779 0.09760221]\n",
      "  P(gw|1,1,1) = [9.99056604e-01 9.43396226e-04]\n",
      "\n",
      "P(ls|he):\n",
      "  P(ls|0) = [0.09996957 0.90003043]\n",
      "  P(ls|1) = [9.99599840e-01 4.00160064e-04]\n",
      "\n",
      "P(vp|pw):\n",
      "  P(vp|0) = [9.99981572e-01 1.84284240e-05]\n",
      "  P(vp|1) = [0.01009698 0.98990302]\n",
      "\n",
      "P(lo|cb):\n",
      "  P(lo|0) = [9.99982890e-01 1.71101035e-05]\n",
      "  P(lo|1) = [0.00103582 0.99896418]\n",
      "\n",
      "P(wv|wr):\n",
      "  P(wv|0) = [0.19985667 0.80014333]\n",
      "  P(wv|1) = [9.99962019e-01 3.79809336e-05]\n",
      "\n",
      "P(hp|dp):\n",
      "  P(hp|0) = [0.09929465 0.90070535]\n",
      "  P(hp|1) = [9.99924196e-01 7.58035173e-05]\n",
      "\n",
      "P(me|gw,gs):\n",
      "  P(me|0,0) = [9.99987818e-01 1.21821969e-05]\n",
      "  P(me|0,1) = [9.99768626e-01 2.31374364e-04]\n",
      "  P(me|1,0) = [0.09908254 0.90091746]\n",
      "  P(me|1,1) = [0.89921242 0.10078758]\n",
      "\n",
      "P(ta|me,fh):\n",
      "  P(ta|0,0) = [9.99990701e-01 9.29903848e-06]\n",
      "  P(ta|0,1) = [9.99696233e-01 3.03766707e-04]\n",
      "  P(ta|1,0) = [0.04966799 0.95033201]\n",
      "  P(ta|1,1) = [9.99777134e-01 2.22866057e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **************************************************************** 2 marks\n",
    "for rv in rvs:\n",
    "    rv[0].fill(1)\n",
    "\n",
    "for i in range(len(dm)):\n",
    "    for rv in rvs:\n",
    "        rv[0][tuple(dm[i, c] for c in rv[2])] += 1\n",
    "    \n",
    "for rv in rvs:\n",
    "    p = rv[0]\n",
    "    p /= numpy.sum(p, axis=0)\n",
    "\n",
    "\n",
    "# Print out the RVs for a sanity check...\n",
    "for rv in rvs:\n",
    "    if len(rv[2])==1:\n",
    "        print('{} = {}'.format(rv[1], rv[0]))\n",
    "    else:\n",
    "        print('{}:'.format(rv[1]))\n",
    "        for i in loop_conditionals(len(rv[2])-1):\n",
    "            print('  P({}|{}) = {}'.format(itn[rv[2][0]], ','.join([str(v) for v in i[1:]]), rv[0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Factor Graph\n",
    "\n",
    "The graphical model has been given as a _Bayesian network_, but computation is much simpler on a _factor graph_ (see slides for details, including a visual version of the below algorithm). Inevitably, the first step is to convert; the algorithm to do so is:\n",
    "\n",
    "1. For each RV we need two nodes: The random variable itself and a factor node. There must be an edge connecting them.\n",
    "2. Each conditional distribution generates as many edges as there are conditional terms. Each edge is between the factor associated with the RV and one of the RVs it is conditioned on.\n",
    "\n",
    "The algorithm itself involves passing messages along the edges, so what we require is a list of edges. Being more specific, the objective is hence to generate that list of edges; an edge is represented by a tuple containing two integers, these being the indices of the nodes that are either side of the edge. For the RVs we can use the indices already provided by `nti` and used in the `rvs` list. As factors are paired with RVs the index of the factor node paired with RV $i$ shall be defined $i+17$ (there are $17$ RVs, so this puts them immediately after the RVs in the same order).\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16 edges\n"
     ]
    }
   ],
   "source": [
    "def rv2fac(i):\n",
    "    \"\"\"Given the index of a random variable node this returns the index of\n",
    "    the corresponding factor node.\"\"\"\n",
    "    return i + 17\n",
    "\n",
    "\n",
    "def fac2rv(i):\n",
    "    \"\"\"Given the index of a factor node this returns the index of\n",
    "    the corresponding random variable node.\"\"\"\n",
    "    return i - 17\n",
    "\n",
    "\n",
    "\n",
    "def calc_edges(rvs):\n",
    "    \"\"\"Uses the rvs array to calculate and return a list of edges,\n",
    "    each as a tuple (index node a, index node b)\"\"\"\n",
    "    edges = []\n",
    "    # **************************************************************** 1 mark\n",
    "    all_rvs = [i[2] for i in rvs]\n",
    "    for arv in all_rvs:\n",
    "        if len(arv) == 2:\n",
    "            edges.append((arv[1], arv[0]))\n",
    "        else:\n",
    "            temp = arv[0]\n",
    "            for i in arv[1:]:\n",
    "                edges.append((i, temp))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "edges = calc_edges(rvs)\n",
    "print('Generated {} edges'.format(len(edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Order\n",
    "\n",
    "Belief propagation works by passing messages over the edges of the factor graph, two messages per edge as there is one in each direction. A message can only be sent from node `a` to node `b` when node `a` has received all of it's messages, __except__ for the message from node `b` to node `a`, which is not needed for that calculation. For this reason we need to convert the list of edges into a list of messages to send, ordered such that the previous constraint is never violated. Note that this naturally involves doubling the number of items in the list as we now have two entries per edge, one per direction.\n",
    "\n",
    "There are many ways to solve this problem and you're welcome to choose any (the below is not the best), but here is a description of an approach that works by simulating message passing where constraints are checked: Generate a list of messages to send (the edges plus the edges with the tuples reversed, to cover both directions) and an empty list, which will ultimately contain the messages in the order they were sent. The algorithm then proceeds by identifying messages in the first list for which the constraint has been satisfied and then moving them to the end of the second list, indicating the message has been sent. This is repeated until the first list is empty and the second full; the second will be in a valid message sending order.\n",
    "\n",
    "Implemented badly the above is both horrifically slow and fiddly to code. One technique to make it faster is to _bounce_ â€” instead of going through the list of messages to send in the same order each time you go forwards and then backwards and then forwards etc. This avoids the scenario where the to send list contains the messages in reverse sending order, such that with each loop you only find one more message that you can send, meaning you have to loop as many times as there are messages. A second technique also makes the code much simpler. Figuring out if you can send a message when the lists are represented as lists is slow â€” you have to loop both. Instead, convert the lists into a pair of nested dictionaries indexed `[message destination][message source]` (order is important!) that contains `True` if the message has been sent (in second list), `False` if it has not (in first list). It's now simple to check if a message can be sent or not; you will still need to generate the list of messages to send as you flip values from `False` to `True` (make sure you get the order right â€” the dictionaries are indexed backwards).\n",
    "\n",
    "Hint:\n",
    "* When converting from edges to dictionaries of flags `defaultdict(dict)` may simplify.\n",
    "* `all(v for v in thing if <condition>)` is valid Python, that returns `True` only if `v` is `True` for every instance that passes the condition.\n",
    "\n",
    "__(3 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7)\n",
      "(1, 7)\n",
      "(7, 8)\n",
      "(2, 8)\n",
      "(8, 9)\n",
      "(3, 9)\n",
      "(5, 9)\n",
      "(0, 10)\n",
      "(7, 11)\n",
      "(8, 12)\n",
      "(3, 13)\n",
      "(5, 14)\n",
      "(9, 15)\n",
      "(4, 15)\n",
      "(15, 16)\n",
      "(6, 16)\n",
      "(7, 0)\n",
      "(7, 1)\n",
      "(8, 7)\n",
      "(8, 2)\n",
      "(9, 8)\n",
      "(9, 3)\n",
      "(9, 5)\n",
      "(10, 0)\n",
      "(11, 7)\n",
      "(12, 8)\n",
      "(13, 3)\n",
      "(14, 5)\n",
      "(15, 9)\n",
      "(15, 4)\n",
      "(16, 15)\n",
      "(16, 6)\n",
      "Generated 0 messages\n"
     ]
    }
   ],
   "source": [
    "def calc_msg_order(edges):\n",
    "    \"\"\"Given a list of edges converts to a list of messages such that\n",
    "    the returned list contains tuples of (source node, destination node).\"\"\"\n",
    "    msgs = []\n",
    "    # **************************************************************** 3 marks\n",
    "    new_list = edges + [i[::-1] for i in edges]\n",
    "    \n",
    "    for i in new_list:\n",
    "        print(i)\n",
    "        \n",
    "    return msgs\n",
    "\n",
    "\n",
    "\n",
    "msg_order = calc_msg_order(edges)\n",
    "print('Generated {} messages'.format(len(msg_order)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Message Passing\n",
    "\n",
    "There are two kinds of node: RVs and factors. The message sent by a random variable is simply all incoming messages multiplied together, except for the message from the direction it is sending a message, i.e.\n",
    "\n",
    "$$M_{s \\rightarrow d}(x) = \\prod_{\\forall n \\cdot n \\neq d} M_{n \\rightarrow s}(x)$$\n",
    "\n",
    "where $s$ (source) is a RV and $d$ (destination) is a factor and $n$ covers the neighbours of $s$ (they must each have sent a message to $s$, and will be factors). $x$ is $0$ or $1$, corresponding to `False` or `True`; messages are functions, conveniently discrete and hence represented as arrays in this case. This equation is exactly what you need to evaluate in `send_rv()`. An almost identical equation is the _belief_, that is the marginal distribution of a RV and the final output of the algorithm:\n",
    "\n",
    "$$B_s(x) = \\prod_{\\forall n} M_{n \\rightarrow s}(x)$$\n",
    "\n",
    "In the interest of laziness `send_rv` should calculate this if the destination is set to `None`; if implemented in the simplest way this will naturally be the case.\n",
    "\n",
    "\n",
    "The messages that factors send are more complicated, because they _factor_ in (sorry) the conditional probability distributions:\n",
    "\n",
    "$$M_{s \\rightarrow d}(x_d) = \\sum_{\\forall x_m \\cdot m \\neq d} P[s,d,\\ldots] \\prod_{\\forall n \\cdot n \\neq d} M_{n \\rightarrow s}(x_n)$$\n",
    "\n",
    "where $P[s,d,\\ldots]$ is the conditional probability distribution, which will naturally be indexed by the source, destination, and any other neighbours ($n$). The switch to $[]$ is to indicate that you should stop thinking of it as a probability distribution when passing messages, as some of the messages make no sense when interpreted as such (but the final beliefs always make sense; it's just the intermediate messages that get weird). The key detail is that this is no longer simple multiplication: Each message is over a different RV (the RV of its source) and hence needs to be multiplied in the correct way. A typical recipe for this is:\n",
    "1. Copy the factor ($P[s,d,\\cdot]$) so it can be used as working storage\n",
    "2. Multiply in each message to the source (unless from destination), using broadcasting to align it with the correct dimension\n",
    "3. Marginalise out all but the RV of the destination node\n",
    "\n",
    "Hints:\n",
    "* Messages are always length 2 vectors, at least in this scenario where everything is a binary RV.\n",
    "* Remember that the factor index in `send_factor` is offset by $17$ from the index of the actual factor it needs to use in `rvs`.\n",
    "* [`einsum()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) makes `send_factor` very elegant, if you dare (can all be done in one, horrifying, line).\n",
    "* Alternatively, [`reshape()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) is easier to understand for multiplication and then [`sum()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html) lets you sum out (marginalise) many axes at the same time, by giving them as a tuple to the `axes` keyword parameter.\n",
    "\n",
    "__(4 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_rv(src, dest, msgs):\n",
    "    \"\"\"Returns the message to send from src (source, always a RV) to dest (destination,\n",
    "    always a factor). msgs is dictionaries within a dictionary such that [d][s]\n",
    "    gets you the message from s to d. Because the message sending order is\n",
    "    correct (well...) you can assume all required entries exist in msgs.\"\"\"\n",
    "\n",
    "    # **************************************************************** 1 mark\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def send_factor(src, dest, msgs, rvs):\n",
    "    \"\"\"Returns the message to send from src (source, always a factor) to dest\n",
    "    (destination, always a RV). msgs is dictionaries within a dictionary such\n",
    "    that [d][s] gets you the message from s to d. Because the message sending\n",
    "    order is correct you can assume all required entries exist in msgs.\n",
    "    rvs is as defined above and contains the relevant conditional distributions\n",
    "    and the names of the dimensions\"\"\"\n",
    "    \n",
    "    # **************************************************************** 3 marks\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief Propagation\n",
    "\n",
    "Once you have the ability to send messages and an order in which to send the messages the rest of the algorithm is a cake walk. The only trick is that if a RV is known (observed) then instead of using `send_rv` whenever a message is sent from it you send the distribution it is known to be instead (`[1,0]` for `False`, `[0,1]` for `true`). The below code implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginals(known):\n",
    "    \"\"\"known is a dictionary, where a random variable index existing as a key in the dictionary\n",
    "    indicates it has been observed. The value obtained using the key is the value the\n",
    "    random variable has been observed as. Returns a 17x2 matrix, such that [rv, 0] is the\n",
    "    probability of random variable rv being False, [rv, 1] the probability of being True.\"\"\"\n",
    "    \n",
    "    # Message storage...\n",
    "    msgs = defaultdict(dict) # [destination][source] -> message\n",
    "    \n",
    "    # Message passing...\n",
    "    for src, dest in msg_order:\n",
    "        if src < 17:\n",
    "            # Random variable...\n",
    "            if src not in known:\n",
    "                msgs[dest][src] = send_rv(src, dest, msgs)\n",
    "            \n",
    "            else:\n",
    "                msgs[dest][src] = numpy.array([0,1] if known[src] else [1,0])\n",
    "        \n",
    "        else:\n",
    "            # Factor...\n",
    "            msgs[dest][src] = send_factor(src, dest, msgs, rvs)\n",
    "    \n",
    "    # Calculate and return beliefs/marginal distributions...\n",
    "    ret = numpy.empty((17,2))\n",
    "    for r in range(ret.shape[0]):\n",
    "        if r not in known:\n",
    "            ret[r,:] = send_rv(r, None, msgs)\n",
    "            ret[r,:] /= ret[r,:].sum() # This needed due to fix numerical stability issues\n",
    "        \n",
    "        else:\n",
    "            ret[r,:] = numpy.array([0,1] if known[r] else [1,0])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "print('Marginals with no observations:') # P(ta = True) = 0.53288705\n",
    "belief = marginals({})\n",
    "for i in range(belief.shape[0]):\n",
    "    print('  P({}) = {}'.format(itn[i], belief[i,:]))\n",
    "print()\n",
    "\n",
    "\n",
    "print('Marginals if ðš ðšŠðšðšŽðš› ðš›ðšŽðšœðšŽðš›ðšŸðš˜ðš’ðš› ðšŽðš–ðš™ðšðš¢:') # P(ta = True) = 0.05732075\n",
    "belief = marginals({nti['wr'] : True})\n",
    "for i in range(belief.shape[0]):\n",
    "    print('  P({}) = {}'.format(itn[i], belief[i,:]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's broken?\n",
    "\n",
    "The below is not worth any marks, but it would be weird to implement an algorithm and then never run it! Simply print out what the most probable broken part of each of the below machines is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair = {}\n",
    "repair['A'] = {nti['me'] : True}\n",
    "repair['B'] = {nti['wv'] : True}\n",
    "repair['C'] = {nti['wv'] : False, nti['lo'] : True}\n",
    "repair['D'] = {nti['hp'] : False, nti['ta'] : False}\n",
    "repair['E'] = {nti['hp'] : True, nti['wv'] : True, nti['vp']: True}\n",
    "\n",
    "# **************************************************************** 0 marks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
